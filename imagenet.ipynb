{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ef7d9694-55f6-44cb-ad66-de8fec1752b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import argparse\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.multiprocessing as mp\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data.distributed\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision import datasets, transforms, models\n",
    "import horovod.torch as hvd\n",
    "import os\n",
    "import torch.nn as nn\n",
    "import math\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7045630b-d384-4a3d-a94f-a2556f56cc1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training settings in Jupyter Notebook\n",
    "train_dir = 'Desktop/project-thesis/tiny-imagenet-200/train'\n",
    "val_dir = 'Desktop/project-thesis/tiny-imagenet-200/val'\n",
    "log_dir = './logs'\n",
    "checkpoint_format = './checkpoint-{epoch}.pth.tar'\n",
    "fp16_allreduce = False\n",
    "use_adasum = False\n",
    "batches_per_allreduce = 1\n",
    "gradient_predivide_factor = 1.0\n",
    "# Custom settings\n",
    "batch_size = 64  # Set your desired batch size\n",
    "val_batch_size = 64  # Set your desired validation batch size\n",
    "epochs = 100  # Set your desired number of epochs\n",
    "base_lr = 0.001  # Set your desired learning rate\n",
    "warmup_epochs = 5  # Keep this as per your needs\n",
    "momentum = 0.9  # Keep this as per your needs\n",
    "wd = 0.00005  # Keep this as per your needs\n",
    "no_cuda = True  # Set to True to disable CUDA\n",
    "seed = 42  # Set your desired seed for reproducibility\n",
    "\n",
    "# Now you can use these variables in your code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "83d99882-1c5a-4cd2-a5e6-3b7dbae49631",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_learning_rate(epoch, batch_idx):\n",
    "    if epoch < warmup_epochs:\n",
    "        epoch += float(batch_idx + 1) / len(train_loader)\n",
    "        lr_adj = 1. / hvd.size() * (epoch * (hvd.size() - 1) / warmup_epochs + 1)\n",
    "    elif epoch < 30:\n",
    "        lr_adj = 1.\n",
    "    elif epoch < 60:\n",
    "        lr_adj = 1e-1\n",
    "    elif epoch < 80:\n",
    "        lr_adj = 1e-2\n",
    "    else:\n",
    "        lr_adj = 1e-3\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = base_lr * hvd.size() * batches_per_allreduce * lr_adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2324d306-046e-4b57-90bb-4dcd34d563bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(output, target):\n",
    "    # get the index of the max log-probability\n",
    "    pred = output.max(1, keepdim=True)[1]\n",
    "    return pred.eq(target.view_as(pred)).cpu().float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ab0f75c9-05b1-4968-90ef-b432d8ee5c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def save_checkpoint(epoch):\n",
    "    if hvd.rank() == 0:\n",
    "        filepath = checkpoint_format.format(epoch=epoch + 1)\n",
    "        state = {\n",
    "            'model': model.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "        }\n",
    "        torch.save(state, filepath)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9a2257d9-45af-449c-b3fd-9bc590118c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Horovod: average metrics from distributed training.\n",
    "class Metric(object):\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.sum = torch.tensor(0.)\n",
    "        self.n = torch.tensor(0.)\n",
    "\n",
    "    def update(self, val):\n",
    "        self.sum += hvd.allreduce(val.detach().cpu(), name=self.name)\n",
    "        self.n += 1\n",
    "\n",
    "    @property\n",
    "    def avg(self):\n",
    "        return self.sum / self.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "772cb05b-245b-4fc9-a19c-9ad7b2944f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    train_sampler.set_epoch(epoch)\n",
    "    train_loss = Metric('train_loss')\n",
    "    train_accuracy = Metric('train_accuracy')\n",
    "\n",
    "    with tqdm(total=len(train_loader),\n",
    "          desc='Train Epoch #{}'.format(epoch + 1),\n",
    "          disable=not verbose) as t:\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            adjust_learning_rate(epoch, batch_idx)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            # Split data into sub-batches of size batch_size\n",
    "            for i in range(0, len(data), batch_size):\n",
    "                data_batch = data[i:i + batch_size]\n",
    "                target_batch = target[i:i + batch_size]\n",
    "                output = model(data_batch)\n",
    "                train_accuracy.update(accuracy(output, target_batch))\n",
    "                loss = F.cross_entropy(output, target_batch)\n",
    "                train_loss.update(loss)\n",
    "                # Average gradients among sub-batches\n",
    "                loss.div_(math.ceil(float(len(data)) / batch_size))\n",
    "                loss.backward()\n",
    "                #if i % 20 == 0:\n",
    "                #    print(f\"Batch {i}: loss={loss}, accuracy={accuracy}\")\n",
    "            # Gradient is applied across all ranks\n",
    "            optimizer.step()\n",
    "            t.set_postfix({'loss': train_loss.avg.item(),\n",
    "                           'accuracy': 100. * train_accuracy.avg.item()})\n",
    "            t.update(1)\n",
    "\n",
    "    if log_writer:\n",
    "        log_writer.add_scalar('train/loss', train_loss.avg, epoch)\n",
    "        log_writer.add_scalar('train/accuracy', train_accuracy.avg, epoch)\n",
    "\n",
    "\n",
    "def validate(epoch):\n",
    "    model.eval()\n",
    "    val_loss = Metric('val_loss')\n",
    "    val_accuracy = Metric('val_accuracy')\n",
    "\n",
    "    with tqdm(total=len(val_loader),\n",
    "              desc='Validate Epoch  #{}'.format(epoch + 1),\n",
    "              disable=not verbose) as t:\n",
    "        with torch.no_grad():\n",
    "            for data, target in val_loader:\n",
    "                output = model(data)\n",
    "\n",
    "                val_loss.update(F.cross_entropy(output, target))\n",
    "                val_accuracy.update(accuracy(output, target))\n",
    "                t.set_postfix({'loss': val_loss.avg.item(),\n",
    "                               'accuracy': 100. * val_accuracy.avg.item()})\n",
    "                t.update(1)\n",
    "\n",
    "    if log_writer:\n",
    "        log_writer.add_scalar('val/loss', val_loss.avg, epoch)\n",
    "        log_writer.add_scalar('val/accuracy', val_accuracy.avg, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7c870e53-5e31-4ac9-b51a-91d709d5a1c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cpu\"\n",
    "# Set up standard ResNet-18 model.\n",
    "model = models.resnet18()\n",
    "# Finetune Final few layers to adjust for tiny imagenet input\n",
    "model.avgpool = nn.AdaptiveAvgPool2d(1)\n",
    "num_features = model.fc.in_features\n",
    "model.fc = nn.Linear(num_features, 200)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "88d963df-2016-423d-ae93-5b4d3b8ac54a",
   "metadata": {},
   "outputs": [],
   "source": [
    "common_transforms = transforms.Compose(\n",
    "    [\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "343ac2da-112e-41b1-8af6-206826fb0499",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets using the defined transforms\n",
    "train_dataset = datasets.ImageFolder(train_dir, transform=common_transforms)\n",
    "val_dataset = datasets.ImageFolder(val_dir, transform=common_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef952331-882e-401b-af47-fb876da99898",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch #4:  37%|███▋      | 584/1563 [1:41:08<143:18:56, 527.00s/it, loss=4.23, accuracy=11.5]"
     ]
    }
   ],
   "source": [
    "allreduce_batch_size = batch_size * batches_per_allreduce\n",
    "\n",
    "hvd.init()\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "# If set > 0, will resume training from a given checkpoint.\n",
    "resume_from_epoch = 0\n",
    "for try_epoch in range(epochs, 0, -1):\n",
    "    if os.path.exists(checkpoint_format.format(epoch=try_epoch)):\n",
    "        resume_from_epoch = try_epoch\n",
    "        break\n",
    "\n",
    "# Horovod: broadcast resume_from_epoch from rank 0 (which will have\n",
    "# checkpoints) to other ranks.\n",
    "resume_from_epoch = hvd.broadcast(torch.tensor(resume_from_epoch), root_rank=0,\n",
    "                                  name='resume_from_epoch').item()\n",
    "\n",
    "# Horovod: print logs on the first worker.\n",
    "verbose = 1 if hvd.rank() == 0 else 0\n",
    "\n",
    "# Horovod: write TensorBoard logs on first worker.\n",
    "log_writer = SummaryWriter(log_dir) if hvd.rank() == 0 else None\n",
    "\n",
    "# Horovod: limit # of CPU threads to be used per worker.\n",
    "torch.set_num_threads(4)\n",
    "\n",
    "kwargs = {'num_workers': 4, 'pin_memory': False}  # No need for pin_memory on CPU\n",
    "# When supported, use 'forkserver' to spawn dataloader workers instead of 'fork'\n",
    "if (kwargs.get('num_workers', 0) > 0 and hasattr(mp, '_supports_context') and\n",
    "        mp._supports_context and 'forkserver' in mp.get_all_start_methods()):\n",
    "    kwargs['multiprocessing_context'] = 'forkserver'\n",
    "\n",
    "train_sampler = torch.utils.data.distributed.DistributedSampler(\n",
    "    train_dataset, num_replicas=hvd.size(), rank=hvd.rank())\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=allreduce_batch_size,\n",
    "    sampler=train_sampler, **kwargs)\n",
    "\n",
    "val_sampler = torch.utils.data.distributed.DistributedSampler(\n",
    "    val_dataset, num_replicas=hvd.size(), rank=hvd.rank())\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=val_batch_size,\n",
    "                                         sampler=val_sampler, **kwargs)\n",
    "\n",
    "\n",
    "lr_scaler = batches_per_allreduce * hvd.size() if not use_adasum else 1\n",
    "\n",
    "\n",
    "# Horovod: scale learning rate by the number of GPUs.\n",
    "optimizer = optim.SGD(model.parameters(),\n",
    "                      lr=(base_lr * lr_scaler),\n",
    "                      momentum=momentum, weight_decay=wd)\n",
    "\n",
    "# Horovod: (optional) compression algorithm.\n",
    "compression = hvd.Compression.fp16 if fp16_allreduce else hvd.Compression.none\n",
    "\n",
    "# Horovod: wrap optimizer with DistributedOptimizer.\n",
    "optimizer = hvd.DistributedOptimizer(\n",
    "    optimizer, named_parameters=model.named_parameters(),\n",
    "    compression=compression,\n",
    "    backward_passes_per_step=batches_per_allreduce,\n",
    "    op=hvd.Adasum if use_adasum else hvd.Average,\n",
    "    gradient_predivide_factor=gradient_predivide_factor)\n",
    "\n",
    "# Restore from a previous checkpoint, if initial_epoch is specified.\n",
    "if resume_from_epoch > 0 and hvd.rank() == 0:\n",
    "    filepath = checkpoint_format.format(epoch=resume_from_epoch)\n",
    "    checkpoint = torch.load(filepath)\n",
    "    model.load_state_dict(checkpoint['model'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "\n",
    "# Horovod: broadcast parameters & optimizer state.\n",
    "hvd.broadcast_parameters(model.state_dict(), root_rank=0)\n",
    "hvd.broadcast_optimizer_state(optimizer, root_rank=0)\n",
    "\n",
    "for epoch in range(resume_from_epoch, epochs):\n",
    "    train(epoch)\n",
    "    validate(epoch)\n",
    "    save_checkpoint(epoch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c3eade-63ee-4bde-85d8-05946c2c9c85",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
